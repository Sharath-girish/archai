
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>archai.algos.xnas.xnas_op &#8212; Archai  documentation</title>
    <link rel="stylesheet" href="../../../../_static/css/klink.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
         
        <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9">
        
        <link href='http://fonts.googleapis.com/css?family=Open+Sans:300,400,700' rel='stylesheet' type='text/css'>
        <link href='http://fonts.googleapis.com/css?family=Droid+Sans+Mono:400,500,700' rel='stylesheet' type='text/css'>
    
  </head><body>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for archai.algos.xnas.xnas_op</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Microsoft Corporation.</span>
<span class="c1"># Licensed under the MIT license.</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Iterable</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">List</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">math</span> <span class="k">as</span> <span class="nn">ma</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">count</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">from</span> <span class="nn">overrides</span> <span class="kn">import</span> <span class="n">overrides</span>

<span class="kn">from</span> <span class="nn">archai.nas.model_desc</span> <span class="kn">import</span> <span class="n">OpDesc</span>
<span class="kn">from</span> <span class="nn">archai.nas.operations</span> <span class="kn">import</span> <span class="n">Op</span>
<span class="kn">from</span> <span class="nn">archai.nas.arch_params</span> <span class="kn">import</span> <span class="n">ArchParams</span>
<span class="kn">from</span> <span class="nn">archai.common.utils</span> <span class="kn">import</span> <span class="n">zip_eq</span>
<span class="kn">from</span> <span class="nn">archai.common.common</span> <span class="kn">import</span> <span class="n">get_conf</span>
<span class="kn">from</span> <span class="nn">archai.common.common</span> <span class="kn">import</span> <span class="n">get_expdir</span>


<span class="c1"># TODO: reduction cell might have output reduced by 2^1=2X due to</span>
<span class="c1">#   stride 2 through input nodes however FactorizedReduce does only</span>
<span class="c1">#   4X reduction. Is this correct?</span>


<div class="viewcode-block" id="XnasOp"><a class="viewcode-back" href="../../../../api/archai.algos.xnas.html#archai.algos.xnas.xnas_op.XnasOp">[docs]</a><span class="k">class</span> <span class="nc">XnasOp</span><span class="p">(</span><span class="n">Op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The output of XnasOp is weighted output of all allowed primitives.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">PRIMITIVES</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s1">&#39;max_pool_3x3&#39;</span><span class="p">,</span>
        <span class="s1">&#39;avg_pool_3x3&#39;</span><span class="p">,</span>
        <span class="s1">&#39;skip_connect&#39;</span><span class="p">,</span>  <span class="c1"># identity</span>
        <span class="s1">&#39;sep_conv_3x3&#39;</span><span class="p">,</span>
        <span class="s1">&#39;sep_conv_5x5&#39;</span><span class="p">,</span>
        <span class="s1">&#39;dil_conv_3x3&#39;</span><span class="p">,</span>
        <span class="s1">&#39;dil_conv_5x5&#39;</span><span class="p">,</span>
        <span class="s1">&#39;none&#39;</span>  <span class="c1"># this must be at the end so top1 doesn&#39;t chose it</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op_desc</span><span class="p">:</span><span class="n">OpDesc</span><span class="p">,</span> <span class="n">arch_params</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="n">ArchParams</span><span class="p">],</span>
                 <span class="n">affine</span><span class="p">:</span><span class="nb">bool</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># assume last PRIMITIVE is &#39;none&#39;</span>
        <span class="k">assert</span> <span class="n">XnasOp</span><span class="o">.</span><span class="n">PRIMITIVES</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;none&#39;</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">_ops</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">primitive</span> <span class="ow">in</span> <span class="n">XnasOp</span><span class="o">.</span><span class="n">PRIMITIVES</span><span class="p">:</span>
            <span class="n">op</span> <span class="o">=</span> <span class="n">Op</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                <span class="n">OpDesc</span><span class="p">(</span><span class="n">primitive</span><span class="p">,</span> <span class="n">op_desc</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">in_len</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">trainables</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
                <span class="n">affine</span><span class="o">=</span><span class="n">affine</span><span class="p">,</span> <span class="n">arch_params</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>

        <span class="c1"># for getting gradients to non-leaf node</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># we do this at the end so that we can capture all arch params registered by</span>
        <span class="c1"># any previous child modules</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setup_arch_params</span><span class="p">(</span><span class="n">arch_params</span><span class="p">)</span>

<div class="viewcode-block" id="XnasOp.update_alphas"><a class="viewcode-back" href="../../../../api/archai.algos.xnas.html#archai.algos.xnas.xnas_op.XnasOp.update_alphas">[docs]</a>    <span class="k">def</span> <span class="nf">update_alphas</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="p">:</span><span class="nb">float</span><span class="p">,</span> <span class="n">current_t</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">total_t</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">grad_clip</span><span class="p">:</span><span class="nb">float</span><span class="p">):</span>       
        <span class="n">grad_flat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="p">)</span>        
        <span class="n">rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad_flat</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">activ</span><span class="p">))</span> <span class="k">for</span> <span class="n">activ</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activs</span><span class="p">])</span>
        <span class="n">exprewards</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta</span> <span class="o">*</span> <span class="n">rewards</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="c1"># NOTE: Will this remain registered?</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">exprewards</span><span class="p">)</span>

        <span class="c1"># weak learner eviction</span>
        <span class="n">conf</span> <span class="o">=</span> <span class="n">get_conf</span><span class="p">()</span>
        <span class="n">to_evict</span> <span class="o">=</span> <span class="n">conf</span><span class="p">[</span><span class="s1">&#39;nas&#39;</span><span class="p">][</span><span class="s1">&#39;search&#39;</span><span class="p">][</span><span class="s1">&#39;xnas&#39;</span><span class="p">][</span><span class="s1">&#39;to_evict&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">to_evict</span><span class="p">:</span>
            <span class="n">theta</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">ma</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">grad_clip</span> <span class="o">*</span> <span class="p">(</span><span class="n">total_t</span> <span class="o">-</span> <span class="n">current_t</span><span class="p">))</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_ops</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">to_keep_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">theta</span>
            <span class="n">num_ops_kept</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">to_keep_mask</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">assert</span> <span class="n">num_ops_kept</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="c1"># zero out the weights which are evicted</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">to_keep_mask</span><span class="p">)</span>

        <span class="c1"># save some debugging info</span>
        <span class="n">expdir</span> <span class="o">=</span> <span class="n">get_expdir</span><span class="p">()</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">expdir</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span> <span class="o">+</span> <span class="s1">&#39;.txt&#39;</span><span class="p">)</span>

        <span class="c1"># save debug info to file</span>
        <span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">alphas</span><span class="p">))</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span></div>


        

    <span class="k">def</span> <span class="nf">_save_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">hook</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hook</span>

<div class="viewcode-block" id="XnasOp.forward"><a class="viewcode-back" href="../../../../api/archai.algos.xnas.html#archai.algos.xnas.xnas_op.XnasOp.forward">[docs]</a>    <span class="nd">@overrides</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activs</span> <span class="o">=</span> <span class="p">[</span><span class="n">op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ops</span><span class="p">]</span>
        <span class="n">numer</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">activ</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">activ</span> <span class="ow">in</span> <span class="n">zip_eq</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activs</span><span class="p">))</span>
        <span class="n">denom</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">numer</span><span class="p">,</span> <span class="n">denom</span><span class="p">)</span>

        <span class="c1"># register hook to save gradients </span>
        <span class="c1"># NOTE: it has to be done every forward call</span>
        <span class="c1"># otherwise the hook doesn&#39;t remain registered</span>
        <span class="c1"># for subsequent loss.backward calls</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pt</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_save_grad</span><span class="p">())</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pt</span></div>

<div class="viewcode-block" id="XnasOp.finalize"><a class="viewcode-back" href="../../../../api/archai.algos.xnas.html#archai.algos.xnas.xnas_op.XnasOp.finalize">[docs]</a>    <span class="nd">@overrides</span>
    <span class="k">def</span> <span class="nf">finalize</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">OpDesc</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]]:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># select except &#39;none&#39; op</span>
            <span class="n">val</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">desc</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ops</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">finalize</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">desc</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">item</span><span class="p">())</span></div>

<div class="viewcode-block" id="XnasOp.can_drop_path"><a class="viewcode-back" href="../../../../api/archai.algos.xnas.html#archai.algos.xnas.xnas_op.XnasOp.can_drop_path">[docs]</a>    <span class="nd">@overrides</span>
    <span class="k">def</span> <span class="nf">can_drop_path</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span></div>

    <span class="k">def</span> <span class="nf">_setup_arch_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arch_params</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="n">ArchParams</span><span class="p">])</span><span class="o">-&gt;</span><span class="kc">None</span><span class="p">:</span>
        <span class="c1"># do we have shared arch params?</span>
        <span class="k">if</span> <span class="n">arch_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># create our own arch params</span>
            <span class="c1"># the alphas are updated by exponentiated gradient descent</span>
            <span class="c1"># and not by gradients from backprop. so we don&#39;t require grad. </span>
            <span class="n">new_p</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">XnasOp</span><span class="o">.</span><span class="n">PRIMITIVES</span><span class="p">)),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">create_arch_params</span><span class="p">([(</span><span class="s1">&#39;alphas&#39;</span><span class="p">,</span> <span class="n">new_p</span><span class="p">)])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">arch_params</span><span class="o">.</span><span class="n">has_kind</span><span class="p">(</span><span class="s1">&#39;alphas&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_arch_params</span><span class="p">(</span><span class="n">arch_params</span><span class="p">)</span>

        <span class="c1"># we store alphas in list so Pytorch don&#39;t register them</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alphas</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">arch_params</span><span class="p">()</span><span class="o">.</span><span class="n">param_by_kind</span><span class="p">(</span><span class="s1">&#39;alphas&#39;</span><span class="p">))</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_alphas</span><span class="p">)</span><span class="o">==</span><span class="mi">1</span></div>
</pre></div>

          </div>
        </div>
      </div>
        <aside>

            
            <a href="../../../../index.html" id="logo" title=Archai><img class="logo" src="../../../../_static/logo.png" width="150px" height="150px" title=Archai /></a>
            
            
            <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../install.html">Installing Archai</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../features.html">Archai Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../blitz.html">Archai - A 30 Minute Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../petridish.html">Petridish - Code Walkthrough</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq.html">Frequently Asked Questions (FAQs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../roadmap.html">Roadmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dir_struct.html">Directory Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/modules.html">APIs</a></li>
</ul>


            
            <ul>
                <li><a href="https://github.com/microsoft/archai">Github</a></li>
            </ul>
            
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </aside>
    
      <div class="clearer"></div>
    </div>
        <div class="footer">
            2020, Microsoft
        </div>

        
  </body>
</html>