__include__: 'darts.yaml' # defaults are loaded from this file

common:
  yaml_log: False
  log_prefix: ''
  toy_mode:
    max_batches: 21
    train_batch: 128
    test_batch: 128
    seed_train_epochs: 2
    post_train_epochs: 2

nas:
  eval:
    final_desc_foldername: '$expdir/model_desc_gallery' # 
    model_desc:
      cell_post_op: 'proj_channels'
      n_cells_multiplier: 2.5
      n_nodes: 4 # number of nodes in a cell if template desc is not provided
      n_reductions: 2 # number of reductions to be applied
      init_node_ch: 36 # num of input/output channels for nodes in 1st cell
      n_cells: 20 # number of cells
      aux_weight: 0.4 # weight for loss from auxiliary towers in test time arch [DEY: What does this mean?]
    loader:
      train_batch: 64
    trainer:
      epochs: 600
  search:
    final_desc_foldername: '$expdir/model_desc_gallery' # the gallery of models that eval will train from scratch
    petridish:
      convex_hull_eps: 0.025 # tolerance 
      max_parent_samples: 10000000 # maximum number of trials in the parent pool sampler function before giving up and returning a random parent model
      max_madd: 200000000 # if any parent model reaches this many multiply-additions then the search is terminated or it it reaches maximum number of parent pool size
      max_parent_models: 80 # if the pool of parent models reaches this size then search is terminated or if it reaches max multiply-adds
    search_iters: 4
    pareto:
      max_cells: 8
      max_reductions: 3
      max_nodes: 1
      enabled: True # if false then there will only be one seed model. if true a number of seed models with different number of cells, reductions and nodes will be used to initialize the search. this provides more coverage of the frontier.
    model_desc:
      n_cells: 3 # minimum number of cells
      n_reductions: 1 # minimum number of reductions
      n_nodes: 1 # starting number of nodes (not used currently)
      cell_post_op: 'proj_channels'
    seed_train:
      trainer:
        epochs: 80 # number of epochs model will be trained before search
      loader:
        train_batch: 64
    post_train:
      trainer:
        epochs: 80 # number of epochs model will be trained after search
      loader:
        train_batch: 64
    trainer:
      l1_alphas:  0.001   # as per paper
      epochs: 80 # number of epochs model will be trained during search
    loader:
      train_batch: 64
      val_ratio: 0.2 #split portion for test set, 0 to 1
